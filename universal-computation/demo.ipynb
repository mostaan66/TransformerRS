{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Transformers as Universal Computation Engines Demo\n",
    "\n",
    "This is a demo notebook illustrating creating a Frozen Pretrained Transformer (FPT) and training on the Bit XOR task, which converges within a couple minutes.\n",
    "\n",
    "arXiv: https://arxiv.org/pdf/2103.05247.pdf\n",
    "\n",
    "Github: https://github.com/kzl/universal-computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset\n",
    "\n",
    "For this demo, we'll look at calculating the elementwise XOR between two randomly generated bitstrings.\n",
    "If you want to play more with the model, feel free to try larger $n$, although it will take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example(n):\n",
    "    bits = np.random.randint(low=0, high=2, size=(2, n))\n",
    "    xor = np.logical_xor(bits[0], bits[1]).astype(np.long)\n",
    "    return bits.reshape((2*n)), xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  String 1: [0 0 1 1 0]\n",
      "  String 2: [0 0 1 0 0]\n",
      "Output XOR: [0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "bits, xor = generate_example(n)\n",
    "\n",
    "print('  String 1:', bits[:n])\n",
    "print('  String 2:', bits[n:])\n",
    "print('Output XOR:', xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the frozen pretrained transformer\n",
    "\n",
    "We simply wrap a pretrained GPT-2 model with linear input and output layers, then freeze the weights of the self-attention and feedforward layers.\n",
    "You can also see what happens using a randomly initialized model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "gpt2 = GPT2Model.from_pretrained('gpt2')  # loads a pretrained GPT-2 base model\n",
    "in_layer = nn.Embedding(2, 768)           # map bit to GPT-2 embedding dim of 768\n",
    "out_layer = nn.Linear(768, 2)             # predict logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in gpt2.named_parameters():\n",
    "    # freeze all parameters except the layernorm and positional embeddings\n",
    "    if 'ln' in name or 'wpe' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "We train the model with stochastic gradient descent on the Bit XOR task.\n",
    "The model should converge within 5000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(gpt2.parameters()) + list(in_layer.parameters()) + list(out_layer.parameters())\n",
    "optimizer = torch.optim.Adam(params)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in (gpt2, in_layer, out_layer):\n",
    "    layer.to(device=device)\n",
    "    layer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 500, Accuracy: 0.6320000129938126\n",
      "Samples: 1000, Accuracy: 0.600000016093254\n",
      "Samples: 1500, Accuracy: 0.6560000130534172\n",
      "Samples: 2000, Accuracy: 0.7320000123977661\n",
      "Samples: 2500, Accuracy: 0.6400000116229058\n",
      "Samples: 3000, Accuracy: 0.6960000142455101\n",
      "Samples: 3500, Accuracy: 0.7640000116825104\n",
      "Samples: 4000, Accuracy: 0.7520000123977661\n",
      "Samples: 4500, Accuracy: 0.7560000121593475\n",
      "Samples: 5000, Accuracy: 0.8280000087618827\n",
      "Samples: 5500, Accuracy: 0.9000000059604645\n",
      "Samples: 6000, Accuracy: 0.9440000027418136\n",
      "Samples: 6500, Accuracy: 0.9520000028610229\n",
      "Final accuracy: 0.9920000004768371\n"
     ]
    }
   ],
   "source": [
    "accuracies = [0]\n",
    "while sum(accuracies[-50:]) / len(accuracies[-50:]) < .99:\n",
    "    x, y = generate_example(n)\n",
    "    x = torch.from_numpy(x).to(device=device, dtype=torch.long)\n",
    "    y = torch.from_numpy(y).to(device=device, dtype=torch.long)\n",
    "    \n",
    "    embeddings = in_layer(x.reshape(1, -1))\n",
    "    hidden_state = gpt2(inputs_embeds=embeddings).last_hidden_state[:,n:]\n",
    "    logits = out_layer(hidden_state)[0]\n",
    "    \n",
    "    loss = loss_fn(logits, y)\n",
    "    accuracies.append((logits.argmax(dim=-1) == y).float().mean().item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if len(accuracies) % 500 == 0:\n",
    "        accuracy = sum(accuracies[-50:]) / len(accuracies[-50:])\n",
    "        print(f'Samples: {len(accuracies)}, Accuracy: {accuracy}')\n",
    "\n",
    "print(f'Final accuracy: {sum(accuracies[-50:]) / len(accuracies[-50:])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing attention map\n",
    "\n",
    "We can visualize the attention map of the first layer: the model learns to attend to the relevant bits for each element in the XOR operation.\n",
    "Note the two consistent diagonal lines for output tokens 5-9 across samples, denoting each position of either string (the pattern is stronger if the model is allowed to train longer or evaluated on more samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in (gpt2, in_layer, out_layer):\n",
    "    layer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  String 1: [0 1 1 0 0]\n",
      "  String 2: [1 1 1 1 1]\n",
      "Prediction: [1 0 0 1 1]\n",
      "Output XOR: [1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "bits, xor = generate_example(n)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.from_numpy(bits).to(device=device, dtype=torch.long)\n",
    "    \n",
    "    embeddings = in_layer(x)\n",
    "    transformer_outputs = gpt2(\n",
    "        inputs_embeds=embeddings,\n",
    "        return_dict=True,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "    logits = out_layer(transformer_outputs.last_hidden_state[n:])\n",
    "    predictions = logits.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print('  String 1:', bits[:n])\n",
    "print('  String 2:', bits[n:])\n",
    "print('Prediction:', predictions)\n",
    "print('Output XOR:', xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff2d5daab10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attentions = transformer_outputs.attentions[0][0]  # first layer, first in batch\n",
    "mean_attentions = attentions.mean(dim=0)           # take the mean over heads\n",
    "mean_attentions = mean_attentions.cpu().numpy()\n",
    "\n",
    "plt.xlabel('Input Tokens', size=16)\n",
    "plt.xticks(range(10), bits)\n",
    "plt.ylabel('Output Tokens', size=16)\n",
    "plt.yticks(range(10), ['*'] * 5 + list(predictions))\n",
    "\n",
    "plt.imshow(mean_attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "\n",
    "As a sanity check, we can see that the model could solve this task without needing to finetune the self-attention layers! The XOR was computed using only the connections already present in GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.0.attn.c_attn.weight is unchanged\n",
      "h.0.attn.c_attn.bias is unchanged\n",
      "h.0.attn.c_proj.weight is unchanged\n",
      "h.0.attn.c_proj.bias is unchanged\n",
      "h.0.mlp.c_fc.weight is unchanged\n",
      "h.0.mlp.c_fc.bias is unchanged\n",
      "h.0.mlp.c_proj.weight is unchanged\n",
      "h.0.mlp.c_proj.bias is unchanged\n",
      "h.1.attn.c_attn.weight is unchanged\n",
      "h.1.attn.c_attn.bias is unchanged\n",
      "h.1.attn.c_proj.weight is unchanged\n",
      "h.1.attn.c_proj.bias is unchanged\n",
      "h.1.mlp.c_fc.weight is unchanged\n",
      "h.1.mlp.c_fc.bias is unchanged\n",
      "h.1.mlp.c_proj.weight is unchanged\n",
      "h.1.mlp.c_proj.bias is unchanged\n",
      "h.2.attn.c_attn.weight is unchanged\n",
      "h.2.attn.c_attn.bias is unchanged\n",
      "h.2.attn.c_proj.weight is unchanged\n",
      "h.2.attn.c_proj.bias is unchanged\n",
      "h.2.mlp.c_fc.weight is unchanged\n",
      "h.2.mlp.c_fc.bias is unchanged\n",
      "h.2.mlp.c_proj.weight is unchanged\n",
      "h.2.mlp.c_proj.bias is unchanged\n",
      "h.3.attn.c_attn.weight is unchanged\n",
      "h.3.attn.c_attn.bias is unchanged\n",
      "h.3.attn.c_proj.weight is unchanged\n",
      "h.3.attn.c_proj.bias is unchanged\n",
      "h.3.mlp.c_fc.weight is unchanged\n",
      "h.3.mlp.c_fc.bias is unchanged\n",
      "h.3.mlp.c_proj.weight is unchanged\n",
      "h.3.mlp.c_proj.bias is unchanged\n",
      "h.4.attn.c_attn.weight is unchanged\n",
      "h.4.attn.c_attn.bias is unchanged\n",
      "h.4.attn.c_proj.weight is unchanged\n",
      "h.4.attn.c_proj.bias is unchanged\n",
      "h.4.mlp.c_fc.weight is unchanged\n",
      "h.4.mlp.c_fc.bias is unchanged\n",
      "h.4.mlp.c_proj.weight is unchanged\n",
      "h.4.mlp.c_proj.bias is unchanged\n",
      "h.5.attn.c_attn.weight is unchanged\n",
      "h.5.attn.c_attn.bias is unchanged\n",
      "h.5.attn.c_proj.weight is unchanged\n",
      "h.5.attn.c_proj.bias is unchanged\n",
      "h.5.mlp.c_fc.weight is unchanged\n",
      "h.5.mlp.c_fc.bias is unchanged\n",
      "h.5.mlp.c_proj.weight is unchanged\n",
      "h.5.mlp.c_proj.bias is unchanged\n",
      "h.6.attn.c_attn.weight is unchanged\n",
      "h.6.attn.c_attn.bias is unchanged\n",
      "h.6.attn.c_proj.weight is unchanged\n",
      "h.6.attn.c_proj.bias is unchanged\n",
      "h.6.mlp.c_fc.weight is unchanged\n",
      "h.6.mlp.c_fc.bias is unchanged\n",
      "h.6.mlp.c_proj.weight is unchanged\n",
      "h.6.mlp.c_proj.bias is unchanged\n",
      "h.7.attn.c_attn.weight is unchanged\n",
      "h.7.attn.c_attn.bias is unchanged\n",
      "h.7.attn.c_proj.weight is unchanged\n",
      "h.7.attn.c_proj.bias is unchanged\n",
      "h.7.mlp.c_fc.weight is unchanged\n",
      "h.7.mlp.c_fc.bias is unchanged\n",
      "h.7.mlp.c_proj.weight is unchanged\n",
      "h.7.mlp.c_proj.bias is unchanged\n",
      "h.8.attn.c_attn.weight is unchanged\n",
      "h.8.attn.c_attn.bias is unchanged\n",
      "h.8.attn.c_proj.weight is unchanged\n",
      "h.8.attn.c_proj.bias is unchanged\n",
      "h.8.mlp.c_fc.weight is unchanged\n",
      "h.8.mlp.c_fc.bias is unchanged\n",
      "h.8.mlp.c_proj.weight is unchanged\n",
      "h.8.mlp.c_proj.bias is unchanged\n",
      "h.9.attn.c_attn.weight is unchanged\n",
      "h.9.attn.c_attn.bias is unchanged\n",
      "h.9.attn.c_proj.weight is unchanged\n",
      "h.9.attn.c_proj.bias is unchanged\n",
      "h.9.mlp.c_fc.weight is unchanged\n",
      "h.9.mlp.c_fc.bias is unchanged\n",
      "h.9.mlp.c_proj.weight is unchanged\n",
      "h.9.mlp.c_proj.bias is unchanged\n",
      "h.10.attn.c_attn.weight is unchanged\n",
      "h.10.attn.c_attn.bias is unchanged\n",
      "h.10.attn.c_proj.weight is unchanged\n",
      "h.10.attn.c_proj.bias is unchanged\n",
      "h.10.mlp.c_fc.weight is unchanged\n",
      "h.10.mlp.c_fc.bias is unchanged\n",
      "h.10.mlp.c_proj.weight is unchanged\n",
      "h.10.mlp.c_proj.bias is unchanged\n",
      "h.11.attn.c_attn.weight is unchanged\n",
      "h.11.attn.c_attn.bias is unchanged\n",
      "h.11.attn.c_proj.weight is unchanged\n",
      "h.11.attn.c_proj.bias is unchanged\n",
      "h.11.mlp.c_fc.weight is unchanged\n",
      "h.11.mlp.c_fc.bias is unchanged\n",
      "h.11.mlp.c_proj.weight is unchanged\n",
      "h.11.mlp.c_proj.bias is unchanged\n"
     ]
    }
   ],
   "source": [
    "fresh_gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "gpt2.to(device='cpu')\n",
    "gpt2_state_dict = gpt2.state_dict()\n",
    "for name, param in fresh_gpt2.named_parameters():\n",
    "    if 'attn' in name or 'mlp' in name:\n",
    "        new_param = gpt2_state_dict[name]\n",
    "        if torch.abs(param.data - new_param.data).sum() > 1e-8:\n",
    "            print(f'{name} was modified')\n",
    "        else:\n",
    "            print(f'{name} is unchanged')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
